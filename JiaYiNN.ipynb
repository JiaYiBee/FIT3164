{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Iris Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Iris Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "#print(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def seed_all(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  #if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_all(1029)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will process the Iris Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = iris.data\n",
    "#print(features)\n",
    "target = iris.target\n",
    "#print(len(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting to 20% test 80% train\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will standardize the values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiaing the number of classes and features for the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_count = x_train.shape[1]\n",
    "print(features_count)\n",
    "classes = len(np.unique(target))\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting them into Pytorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building up the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dnn_model = Sequential(Linear(features_count,10), nn.ReLU(),\n",
    "                       Linear(10,20), nn.ReLU(),\n",
    "                       Linear(20,15), nn.ReLU(),\n",
    "                       Linear(15, classes)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(model, X_data, y_data, device):\n",
    "    model.eval() #Evaluation Model\n",
    "    with torch.no_grad(): #Disable gradient calculation to save computation energy \n",
    "        X_data, y_data = X_data.to(device), y_data.to(device)\n",
    "        #use the model to predict the results\n",
    "        outputs = model(X_data.type(torch.float32))  \n",
    "        #choose the highest probability for each row so it belongs to the certain class\n",
    "        predicted = torch.argmax(outputs.data, 1)\n",
    "        corrects = (predicted == y_data.type(torch.long)).sum().item()\n",
    "        totals = y_data.size(0)\n",
    "        acc = float(corrects) / totals\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to create model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "\tdnn_model = Sequential(Linear(features_count,10), nn.ReLU(),\n",
    "                         Linear(10,20), nn.ReLU(),\n",
    "                         Linear(20,15), nn.ReLU(),\n",
    "                         Linear(15, classes))\n",
    "\treturn dnn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model=None, X_train=None, y_train=None, loss_fn=None, optimizer=torch.optim.Adam,\n",
    "        learning_rate=0.001, num_epochs=100, verbose=True, seed=1234, device=None):\n",
    "    torch.manual_seed(seed)\n",
    "    optim = optimizer(model.parameters(), lr=learning_rate)\n",
    "    history = dict()\n",
    "    history['train_loss'] = []\n",
    "    history['train_acc'] = []\n",
    "\n",
    "    # Move data to device\n",
    "    X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        #allow the model to go in training mode\n",
    "        model.train()\n",
    "        \n",
    "        #foward propaogation\n",
    "        outputs = model(X_train.type(torch.float32))\n",
    "        loss = loss_fn(outputs, y_train.type(torch.long))\n",
    "        \n",
    "        #make sure the gradient computed is zero so it doesnt accumulate from previous iteration\n",
    "        optim.zero_grad()\n",
    "        #conpute the gradient lost\n",
    "        loss.backward()\n",
    "        #update the weights\n",
    "        optim.step()\n",
    "\n",
    "        #evaluate the accuracy for this current epoch\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        train_loss = compute_loss(model, loss_fn, X_train, y_train, device)\n",
    "        train_acc = compute_acc(model, X_train, y_train, device)\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"train loss= {train_loss:.4f} - train acc= {train_acc*100:.2f}%\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model and choosing the optimiser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "optim_dict = {\"Adam\":optim.Adam, \"Adadelta\":optim.Adadelta, \"Adagrad\":optim.Adagrad,\n",
    "              \"Adamax\":optim.Adamax, \"AdamW\": optim.AdamW, \"ASGD\":optim.ASGD,\n",
    "              \"NAdam\":optim.NAdam, \"RMSprop\":optim.RMSprop, \"RAdam\":optim.RAdam,\n",
    "              \"Rprop\": optim.Rprop, \"SGD\":optim.SGD}\n",
    "\n",
    "dnn_model = create_model().to(device)\n",
    "history = fit(dnn_model, X_train = x_train_tensor, y_train=y_train_tensor, loss_fn = nn.CrossEntropyLoss(),\n",
    "    optimizer = optim_dict[\"SGD\"], learning_rate = 0.1, num_epochs = 50, verbose= True, seed=123, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wrong(model, X_data, y_data, device):\n",
    "    model.eval() #Evaluation Model\n",
    "    with torch.no_grad(): #Disable gradient calculation to save computation energy \n",
    "        X_data, y_data = X_data.to(device), y_data.to(device)\n",
    "        #use the model to predict the results\n",
    "        outputs = model(X_data.type(torch.float32))  \n",
    "        #choose the highest probability for each row so it belongs to the certain class\n",
    "        predicted = torch.argmax(outputs.data, 1)\n",
    "        #print(\"Actual:   \", y_data)\n",
    "        #print(\"Predicted:\", predicted)\n",
    "        corrects = (predicted == y_data.type(torch.long)).sum().item()\n",
    "        incorrects = (predicted != y_data.type(torch.long)).sum().item()\n",
    "        totals = y_data.size(0)\n",
    "        acc = float(corrects) / totals\n",
    "        #0 = Iris Setosa, 1 = Iris Versicolor , 2 = Iris Virginica\n",
    "        for i in range(len(y_data)):\n",
    "            if predicted[i] != y_data[i]:\n",
    "                #if we predict it is setosa\n",
    "                if predicted[i] == 0:\n",
    "                    if y_data[i] == 1:\n",
    "                        print(\"Predicted : Iris Setosa\", \"|| Correct : Iris Versicolor\")\n",
    "                    \n",
    "                    else:\n",
    "                        print(\"Predicted : Iris Setosa\", \"|| Correct : Iris Virginica\")\n",
    "                        \n",
    "                #if we predict it is versicolor\n",
    "                elif predicted[i] == 1:\n",
    "                    if y_data[i] == 0:\n",
    "                        print(\"Predicted : Iris Versicolor \", \"|| Correct : Iris Setosa\")\n",
    "                    \n",
    "                    else:\n",
    "                        print(\"Predicted : Iris Versicolor\", \"|| Correct : Iris Virginica\")\n",
    "                #if we predict it is virginica\n",
    "                else:\n",
    "                    if y_data[i] == 0:\n",
    "                        print(\"Predicted : Iris Virginica\", \"|| Correct : Iris Setosa\")\n",
    "                    \n",
    "                    else:\n",
    "                        print(\"Predicted : Iris Virginica\", \"|| Correct : Iris Versicolor\")\n",
    "        \n",
    "                    \n",
    "        \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = compute_wrong(dnn_model, x_test_tensor, y_test_tensor, device)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pretty much see that the model is having a hard time differentiating between Iris Virginica and Iris Versicolor, this might be caused by the similaries of petal length, petal width and speal length and sepal width."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to investigate whether if the number of epoch would affect the accuracy and how would the time be affected too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def number_of_epoches():\n",
    "    accuracy_hist = []\n",
    "    time_taken = []\n",
    "    num_epoch = [20,30,40,50,60,70]\n",
    "    dnn_model = Sequential(Linear(features_count,10), nn.ReLU(),\n",
    "                         Linear(10,20), nn.ReLU(),\n",
    "                         Linear(20,15), nn.ReLU(),\n",
    "                         Linear(15, classes))\n",
    "    \n",
    "    for epochs in num_epoch:\n",
    "        starttime  = time.time()\n",
    "        history = fit(dnn_model, X_train=x_train_tensor, y_train=y_train_tensor, loss_fn=nn.CrossEntropyLoss(),\n",
    "                      optimizer=optim_dict[\"SGD\"], learning_rate=0.1, num_epochs=epochs, verbose=False, seed=101, device=device)\n",
    "        endtime = time.time()\n",
    "        elapsed_time = endtime - starttime\n",
    "        time_taken.append(elapsed_time)\n",
    "        \n",
    "        #saving the accuracies\n",
    "        accuracy_hist.append(compute_acc(dnn_model, x_test_tensor, y_test_tensor, device))\n",
    "    \n",
    "    \n",
    "    #plotting for Accuracy vs Epoch\n",
    "    plt.plot(num_epoch, accuracy_hist, 'bo-', label='Accuracy vs Number of Epochs')\n",
    "    plt.xlabel('Number of Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs Number of Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    #plotting for Time vs Neurons\n",
    "    plt.plot(num_epoch, time_taken, 'ro-', label='Time vs Number of Epochs')\n",
    "    plt.xlabel('Number of Epochs')\n",
    "    plt.ylabel('Time')\n",
    "    plt.title('Time vs Number of Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    return None\n",
    "    \n",
    "number_of_epoches() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to see if we can reduce the computation energy by reducing the hidden layer to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With one hidden layer only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_one_layer():\n",
    "\tdnn_model = Sequential(Linear(features_count,10), nn.ReLU(),\n",
    "                         Linear(10, classes))\n",
    "\treturn dnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model = create_model_one_layer().to(device)\n",
    "history = fit(dnn_model, X_train = x_train_tensor, y_train=y_train_tensor, loss_fn = nn.CrossEntropyLoss(),\n",
    "optimizer = optim_dict[\"SGD\"], learning_rate = 0.1, num_epochs = 60, verbose= False, seed=101, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = compute_wrong(dnn_model, x_test_tensor, y_test_tensor, device)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that even with one layer, we could still perform well, we could conserve computation energy by just using one layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to see how the number of neurons for a layer affect the accuracies and the time for computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def number_of_neurons_a_layer():\n",
    "    accuracy_hist = []\n",
    "    time_taken = []\n",
    "    num_neurons = [5,10,15,20,25,30]\n",
    "    models = [\n",
    "            Sequential(Linear(features_count, 5), nn.ReLU(), Linear(5, classes)),\n",
    "            Sequential(Linear(features_count, 10), nn.ReLU(), Linear(10, classes)),\n",
    "            Sequential(Linear(features_count, 15), nn.ReLU(), Linear(15, classes)),\n",
    "            Sequential(Linear(features_count, 20), nn.ReLU(), Linear(20, classes)),\n",
    "            Sequential(Linear(features_count, 25), nn.ReLU(), Linear(25, classes)),\n",
    "            Sequential(Linear(features_count, 30), nn.ReLU(), Linear(30, classes)),\n",
    "    ]\n",
    "    \n",
    "    for model in models:\n",
    "        starttime  = time.time()\n",
    "        history = fit(model, X_train=x_train_tensor, y_train=y_train_tensor, loss_fn=nn.CrossEntropyLoss(),\n",
    "                      optimizer=optim_dict[\"SGD\"], learning_rate=0.1, num_epochs=60, verbose=False, seed=101, device=device)\n",
    "        endtime = time.time()\n",
    "        elapsed_time = endtime - starttime\n",
    "        time_taken.append(elapsed_time)\n",
    "        \n",
    "        #saving the accuracies\n",
    "        accuracy_hist.append(compute_acc(model, x_test_tensor, y_test_tensor, device))\n",
    "    \n",
    "    \n",
    "    #plotting for Accuracy vs Neurons\n",
    "    plt.plot(num_neurons, accuracy_hist, 'bo-', label='Accuracy vs Number of Neurons')\n",
    "    plt.xlabel('Number of Neurons')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs Number of Neurons')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    #plotting for Time vs Neurons\n",
    "    plt.plot(num_neurons, time_taken, 'ro-', label='Accuracy vs Number of Neurons')\n",
    "    plt.xlabel('Number of Neurons')\n",
    "    plt.ylabel('Time') \n",
    "    plt.title('Time vs Number of Neurons')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    return None\n",
    "    \n",
    "number_of_neurons_a_layer() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, most of the time, with higher number of neurons, we are able to get higher accuracies, however, the time needed for computation also increases which is a trade off. This is due to more number of neurons would be able to **learn more complex patterns** , **avoids underfitting** and also **introduce more non-linear combinations learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Now we will try changing the final activation function to Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would create something called OneVersusAllLoss as we are using sigmoid for multiclass classification problem. What we do is we basically build a custom foward propagation for our model.\n",
    "The details are as follows:\n",
    "- Now we are using the `the sigmoid activation function` to logits instead of `the softmax activation` function as what we did for the previous models to obtain probabilities which sums up to 1, meaning that $p_i = sigmoid(h_i), i=1,...,M$. Where $M$ is the number of classes which is 3 here.\n",
    "- Given a data example $x$ with the ground-truth label $y$, the idea is to maximize the likelihood $p_y$ to be predicted as 1 and to minimize the likelihoods $p_i, i \\neq y$. So what we need to do is:\n",
    "  - $\\min\\left\\{ -\\log p_{y}-\\sum_{i\\neq y}\\log(1-p_{i})\\right\\}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneVersusAllLoss(nn.Module):\n",
    "    def forward(self, predictions, targets):\n",
    "        #converts the outputs from logits to sigmoids probs\n",
    "        probabilities = torch.sigmoid(predictions)\n",
    "        \n",
    "        #encoding the targets so they are one hot encoded\n",
    "        #create the place holder\n",
    "        targets_one_hot = torch.zeros_like(probabilities)\n",
    "        \n",
    "        #after changing the dimension of the targets to (batch,1)\n",
    "        #we would then place all of the 1 in the acording place by the (batch,1)\n",
    "        targets_one_hot.scatter_(1, targets.unsqueeze(1), 1)\n",
    "        \n",
    "        #we need to calculate the probability for the positive, the correct class\n",
    "        log_prob_py = torch.log(probabilities + 1e-10) #avoiding log 0\n",
    "        \n",
    "        #only extract the true class, if multiply other class which is 0 will give 0\n",
    "        log_probs_true = torch.sum(targets_one_hot * log_prob_py, dim=1)\n",
    "    \n",
    "        #now we need to extract the negative class excluding the positive class\n",
    "        log_nonpy = torch.log(1 - probabilities + 1e-10) #avoiding log 0\n",
    "        \n",
    "        #only summing the prob of the negative class as 1 - one hot removes the true class label 1\n",
    "        log_probs_false = torch.sum((1 - targets_one_hot) * log_nonpy, dim=1)\n",
    "        \n",
    "        #total ova lost acording to the question which is the log of true and log of false\n",
    "        loss = -(log_probs_true + log_probs_false)\n",
    "        \n",
    "        return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training it with activation sigmoid function at the last layer to see the performance instead of applying softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_sigmoid_iris(model=None, X_train=None, y_train=None, loss_fn=None, optimizer=torch.optim.Adam,\n",
    "        learning_rate=0.001, num_epochs=100, verbose=True, seed=123, device=None):\n",
    "    torch.manual_seed(seed)\n",
    "    optim = optimizer(model.parameters(), lr=learning_rate)\n",
    "    history = dict()\n",
    "    history['train_loss'] = []\n",
    "    history['train_acc'] = []\n",
    "\n",
    "    # Move data to device\n",
    "    X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        #allow the model to go in training mode\n",
    "        model.train()\n",
    "        \n",
    "        #foward propaogation\n",
    "        outputs = model(X_train.type(torch.float32))\n",
    "        #the softmax cross entropy\n",
    "        loss = loss_fn(outputs, y_train.type(torch.long))\n",
    "        #print(loss)\n",
    "        \n",
    "        #make sure the gradient computed is zero so it doesnt accumulate from previous iteration\n",
    "        optim.zero_grad()\n",
    "        #conpute the gradient lost\n",
    "        loss.backward()\n",
    "        #update the weights\n",
    "        optim.step()\n",
    "\n",
    "        #evaluate the accuracy for this current epoch\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        train_loss = compute_loss(model, loss_fn, X_train, y_train, device)\n",
    "        train_acc = compute_acc(model, X_train, y_train, device)\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"train loss= {train_loss:.4f} - train acc= {train_acc*100:.2f}%\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "\n",
    "sigmoid_model = create_model().to(device)\n",
    "history = fit_sigmoid_iris(sigmoid_model, X_train = x_train_tensor, y_train=y_train_tensor, loss_fn = OneVersusAllLoss(),\n",
    "    optimizer = optim_dict[\"SGD\"], learning_rate = 0.15, num_epochs = 60, verbose= True, seed=101, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = compute_wrong(sigmoid_model, x_test_tensor, y_test_tensor, device)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It performs quite well and it is almost the same as our original model where we applied the softmax at the last layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boostrapping stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would boostrap our data set and run it with the default 60 epoch to see if our model works well with the boostraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a data frame\n",
    "#numpy boostrap\n",
    "def bootstrap_numpy(x, y, n_samples):\n",
    "    #choosing a random indicies, indices can be repeated, which means that it is with replacement\n",
    "    indices = np.random.choice(np.arange(len(x)), size=n_samples, replace=True)\n",
    "    x_bootstrap = x[indices]\n",
    "    y_bootstrap = y[indices]\n",
    "    return x_bootstrap, y_bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multiple bootstrap samples\n",
    "n_iterations = 10\n",
    "#creating a bootstrap sample with the same size as the originial data set\n",
    "bootstrap_samples = [bootstrap_numpy(features, target, len(features)) for _ in range(n_iterations)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after we have created 10 boostrap samples which is sampled from the original data set, now we would create a statiscal graph to allow us ot see how well our model perform over this 10 data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(bootstrap_samples)):\n",
    "    features_boot = bootstrap_samples[i][0]\n",
    "    target_boot = bootstrap_samples[i][1]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(features_boot, target_boot, test_size=0.2)\n",
    "    #setting some important parameters\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "    features_count = x_train.shape[1]\n",
    "    classes = len(np.unique(target_boot))\n",
    "    \n",
    "    #convert to pytorch tensor\n",
    "    x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "    \n",
    "    #call the fit to compute the accuracies and save it\n",
    "    bootstrap_model = create_model().to(device)\n",
    "    history = fit(bootstrap_model, X_train = x_train_tensor, y_train=y_train_tensor, loss_fn = nn.CrossEntropyLoss(),\n",
    "                  optimizer = optim_dict[\"SGD\"], learning_rate = 0.15, num_epochs = 60, verbose= False, seed=101, device=device)\n",
    "    test_accuracy = compute_wrong(dnn_model, x_test_tensor, y_test_tensor, device)\n",
    "    print(f\"Test Accuracy: {test_accuracy * 100:.2f}% for bootstrap sample {i+1} \")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the bootstrap accuracies, we can pretty much see that the accuracies are almost always above 80% , and we can also see that Iris Virginica are commonly mistaken with Iris Versicolor due to the nature of their similar features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
