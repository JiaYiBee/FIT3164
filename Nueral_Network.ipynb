{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Iris Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Iris Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "#print(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def seed_all(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  #if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_all(1029)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will process the Iris Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = iris.data\n",
    "#print(features)\n",
    "target = iris.target\n",
    "#print(len(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting to 20% test 80% train\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will standardize the values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiaing the number of classes and features for the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_count = x_train.shape[1]\n",
    "print(features_count)\n",
    "classes = len(np.unique(target))\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting them into Pytorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building up the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dnn_model = Sequential(Linear(features_count,10), nn.ReLU(),\n",
    "                       Linear(10,20), nn.ReLU(),\n",
    "                       Linear(20,15), nn.ReLU(),\n",
    "                       Linear(15, classes)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(model, X_data, y_data, device):\n",
    "    model.eval() #Evaluation Model\n",
    "    with torch.no_grad(): #Disable gradient calculation to save computation energy \n",
    "        X_data, y_data = X_data.to(device), y_data.to(device)\n",
    "        #use the model to predict the results\n",
    "        outputs = model(X_data.type(torch.float32))  \n",
    "        #choose the highest probability for each row so it belongs to the certain class\n",
    "        predicted = torch.argmax(outputs.data, 1)\n",
    "        corrects = (predicted == y_data.type(torch.long)).sum().item()\n",
    "        totals = y_data.size(0)\n",
    "        acc = float(corrects) / totals\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to create model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "\tdnn_model = Sequential(Linear(features_count,10), nn.ReLU(),\n",
    "                         Linear(10,20), nn.ReLU(),\n",
    "                         Linear(20,15), nn.ReLU(),\n",
    "                         Linear(15, classes))\n",
    "\treturn dnn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model=None, X_train=None, y_train=None, loss_fn=None, optimizer=torch.optim.Adam,\n",
    "        learning_rate=0.001, num_epochs=100, verbose=True, seed=1234, device=None):\n",
    "    torch.manual_seed(seed)\n",
    "    optim = optimizer(model.parameters(), lr=learning_rate)\n",
    "    history = dict()\n",
    "    history['train_loss'] = []\n",
    "    history['train_acc'] = []\n",
    "\n",
    "    # Move data to device\n",
    "    X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        #allow the model to go in training mode\n",
    "        model.train()\n",
    "        \n",
    "        #foward propaogation\n",
    "        outputs = model(X_train.type(torch.float32))\n",
    "        loss = loss_fn(outputs, y_train.type(torch.long))\n",
    "        \n",
    "        #make sure the gradient computed is zero so it doesnt accumulate from previous iteration\n",
    "        optim.zero_grad()\n",
    "        #conpute the gradient lost\n",
    "        loss.backward()\n",
    "        #update the weights\n",
    "        optim.step()\n",
    "\n",
    "        #evaluate the accuracy for this current epoch\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        train_loss = compute_loss(model, loss_fn, X_train, y_train, device)\n",
    "        train_acc = compute_acc(model, X_train, y_train, device)\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"train loss= {train_loss:.4f} - train acc= {train_acc*100:.2f}%\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model and choosing the optimiser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "optim_dict = {\"Adam\":optim.Adam, \"Adadelta\":optim.Adadelta, \"Adagrad\":optim.Adagrad,\n",
    "              \"Adamax\":optim.Adamax, \"AdamW\": optim.AdamW, \"ASGD\":optim.ASGD,\n",
    "              \"NAdam\":optim.NAdam, \"RMSprop\":optim.RMSprop, \"RAdam\":optim.RAdam,\n",
    "              \"Rprop\": optim.Rprop, \"SGD\":optim.SGD}\n",
    "\n",
    "dnn_model = create_model().to(device)\n",
    "history = fit(dnn_model, X_train = x_train_tensor, y_train=y_train_tensor, loss_fn = nn.CrossEntropyLoss(),\n",
    "    optimizer = optim_dict[\"SGD\"], learning_rate = 0.1, num_epochs = 50, verbose= True, seed=123, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wrong(model, X_data, y_data, device):\n",
    "    model.eval() #Evaluation Model\n",
    "    with torch.no_grad(): #Disable gradient calculation to save computation energy \n",
    "        X_data, y_data = X_data.to(device), y_data.to(device)\n",
    "        #use the model to predict the results\n",
    "        outputs = model(X_data.type(torch.float32))  \n",
    "        #choose the highest probability for each row so it belongs to the certain class\n",
    "        predicted = torch.argmax(outputs.data, 1)\n",
    "        #print(\"Actual:   \", y_data)\n",
    "        #print(\"Predicted:\", predicted)\n",
    "        corrects = (predicted == y_data.type(torch.long)).sum().item()\n",
    "        incorrects = (predicted != y_data.type(torch.long)).sum().item()\n",
    "        totals = y_data.size(0)\n",
    "        acc = float(corrects) / totals\n",
    "        #0 = Iris Setosa, 1 = Iris Versicolor , 2 = Iris Virginica\n",
    "        for i in range(len(y_data)):\n",
    "            if predicted[i] != y_data[i]:\n",
    "                #if we predict it is setosa\n",
    "                if predicted[i] == 0:\n",
    "                    if y_data[i] == 1:\n",
    "                        print(\"Predicted : Iris Setosa\", \"|| Correct : Iris Versicolor\")\n",
    "                    \n",
    "                    else:\n",
    "                        print(\"Predicted : Iris Setosa\", \"|| Correct : Iris Virginica\")\n",
    "                        \n",
    "                #if we predict it is versicolor\n",
    "                elif predicted[i] == 1:\n",
    "                    if y_data[i] == 0:\n",
    "                        print(\"Predicted : Iris Versicolor \", \"|| Correct : Iris Setosa\")\n",
    "                    \n",
    "                    else:\n",
    "                        print(\"Predicted : Iris Versicolor\", \"|| Correct : Iris Virginica\")\n",
    "                #if we predict it is virginica\n",
    "                else:\n",
    "                    if y_data[i] == 0:\n",
    "                        print(\"Predicted : Iris Virginica\", \"|| Correct : Iris Setosa\")\n",
    "                    \n",
    "                    else:\n",
    "                        print(\"Predicted : Iris Virginica\", \"|| Correct : Iris Versicolor\")\n",
    "        \n",
    "                    \n",
    "        \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = compute_wrong(dnn_model, x_test_tensor, y_test_tensor, device)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pretty much see that the model is having a hard time differentiating between Iris Virginica and Iris Versicolor, this might be caused by the similaries of petal length, petal width and speal length and sepal width."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to investigate whether if the number of epoch would affect the accuracy and how would the time be affected too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def number_of_epoches():\n",
    "    accuracy_hist = []\n",
    "    time_taken = []\n",
    "    num_epoch = [20,30,40,50,60,70]\n",
    "    dnn_model = Sequential(Linear(features_count,10), nn.ReLU(),\n",
    "                         Linear(10,20), nn.ReLU(),\n",
    "                         Linear(20,15), nn.ReLU(),\n",
    "                         Linear(15, classes))\n",
    "    \n",
    "    for epochs in num_epoch:\n",
    "        starttime  = time.time()\n",
    "        history = fit(dnn_model, X_train=x_train_tensor, y_train=y_train_tensor, loss_fn=nn.CrossEntropyLoss(),\n",
    "                      optimizer=optim_dict[\"SGD\"], learning_rate=0.1, num_epochs=epochs, verbose=False, seed=101, device=device)\n",
    "        endtime = time.time()\n",
    "        elapsed_time = endtime - starttime\n",
    "        time_taken.append(elapsed_time)\n",
    "        \n",
    "        #saving the accuracies\n",
    "        accuracy_hist.append(compute_acc(dnn_model, x_test_tensor, y_test_tensor, device))\n",
    "    \n",
    "    \n",
    "    #plotting for Accuracy vs Epoch\n",
    "    plt.plot(num_epoch, accuracy_hist, 'bo-', label='Accuracy vs Number of Epochs')\n",
    "    plt.xlabel('Number of Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs Number of Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    #plotting for Time vs Neurons\n",
    "    plt.plot(num_epoch, time_taken, 'ro-', label='Time vs Number of Epochs')\n",
    "    plt.xlabel('Number of Epochs')\n",
    "    plt.ylabel('Time')\n",
    "    plt.title('Time vs Number of Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    return None\n",
    "    \n",
    "number_of_epoches() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to see if we can reduce the computation energy by reducing the hidden layer to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With one hidden layer only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_one_layer():\n",
    "\tdnn_model = Sequential(Linear(features_count,10), nn.ReLU(),\n",
    "                         Linear(10, classes))\n",
    "\treturn dnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model = create_model_one_layer().to(device)\n",
    "history = fit(dnn_model, X_train = x_train_tensor, y_train=y_train_tensor, loss_fn = nn.CrossEntropyLoss(),\n",
    "optimizer = optim_dict[\"SGD\"], learning_rate = 0.1, num_epochs = 60, verbose= False, seed=101, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = compute_wrong(dnn_model, x_test_tensor, y_test_tensor, device)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that even with one layer, we could still perform well, we could conserve computation energy by just using one layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to see how the number of neurons for a layer affect the accuracies and the time for computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def number_of_neurons_a_layer():\n",
    "    accuracy_hist = []\n",
    "    time_taken = []\n",
    "    num_neurons = [5,10,15,20,25,30]\n",
    "    models = [\n",
    "            Sequential(Linear(features_count, 5), nn.ReLU(), Linear(5, classes)),\n",
    "            Sequential(Linear(features_count, 10), nn.ReLU(), Linear(10, classes)),\n",
    "            Sequential(Linear(features_count, 15), nn.ReLU(), Linear(15, classes)),\n",
    "            Sequential(Linear(features_count, 20), nn.ReLU(), Linear(20, classes)),\n",
    "            Sequential(Linear(features_count, 25), nn.ReLU(), Linear(25, classes)),\n",
    "            Sequential(Linear(features_count, 30), nn.ReLU(), Linear(30, classes)),\n",
    "    ]\n",
    "    \n",
    "    for model in models:\n",
    "        starttime  = time.time()\n",
    "        history = fit(model, X_train=x_train_tensor, y_train=y_train_tensor, loss_fn=nn.CrossEntropyLoss(),\n",
    "                      optimizer=optim_dict[\"SGD\"], learning_rate=0.1, num_epochs=60, verbose=False, seed=101, device=device)\n",
    "        endtime = time.time()\n",
    "        elapsed_time = endtime - starttime\n",
    "        time_taken.append(elapsed_time)\n",
    "        \n",
    "        #saving the accuracies\n",
    "        accuracy_hist.append(compute_acc(model, x_test_tensor, y_test_tensor, device))\n",
    "    \n",
    "    \n",
    "    #plotting for Accuracy vs Neurons\n",
    "    plt.plot(num_neurons, accuracy_hist, 'bo-', label='Accuracy vs Number of Neurons')\n",
    "    plt.xlabel('Number of Neurons')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs Number of Neurons')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    #plotting for Time vs Neurons\n",
    "    plt.plot(num_neurons, time_taken, 'ro-', label='Accuracy vs Number of Neurons')\n",
    "    plt.xlabel('Number of Neurons')\n",
    "    plt.ylabel('Time') \n",
    "    plt.title('Time vs Number of Neurons')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    return None\n",
    "    \n",
    "number_of_neurons_a_layer() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, most of the time, with higher number of neurons, we are able to get higher accuracies, however, the time needed for computation also increases which is a trade off. This is due to more number of neurons would be able to **learn more complex patterns** , **avoids underfitting** and also **introduce more non-linear combinations learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
